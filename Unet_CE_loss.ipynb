{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RRCGDDataset(Dataset):\n",
    "    def __init__(self, rrcgd_dir, label_dir):\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(rrcgd_dir)\n",
    "            if f.endswith(\"_rrcgd.npy\")\n",
    "        ])\n",
    "        self.rrcgd_dir = rrcgd_dir\n",
    "        self.label_dir = label_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rrcgd_path = os.path.join(self.rrcgd_dir, self.files[idx])\n",
    "        label_path = os.path.join(\n",
    "            self.label_dir,\n",
    "            self.files[idx].replace(\"_rrcgd.npy\", \".npy\")\n",
    "        )\n",
    "\n",
    "        X = np.load(rrcgd_path)              # (T, 256)\n",
    "        y_hz = np.load(label_path)        # (T,)\n",
    "\n",
    "        y = hz_to_bin(y_hz)                  # (T,)\n",
    "        return torch.from_numpy(X), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs, Ys = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor([x.shape[0] for x in Xs])\n",
    "\n",
    "    Xs = pad_sequence(Xs, batch_first=True)  # (B, T, 256)\n",
    "    Ys = pad_sequence(Ys, batch_first=True)  # (B, T)\n",
    "\n",
    "    mask = (Ys != 0).float()                 # voiced mask\n",
    "\n",
    "    return Xs, Ys, mask, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec115c68-78a4-4f20-ba1a-e71b54bdee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import mir_eval\n",
    "\n",
    "F_REF = 55.0\n",
    "C_MIN = -1200.0\n",
    "CENTS_PER_BIN = 20.0\n",
    "def bin_to_hz(bins: np.ndarray):\n",
    "    \"\"\"\n",
    "    bins: (T,) int, 0 = unvoiced\n",
    "    returns: (T,) float Hz, 0 = unvoiced\n",
    "    \"\"\"\n",
    "    hz = np.zeros_like(bins, dtype=np.float32)\n",
    "    voiced = bins > 0\n",
    "\n",
    "    cents = C_MIN + bins[voiced] * CENTS_PER_BIN\n",
    "    hz[voiced] = F_REF * (2.0 ** (cents / 1200.0))\n",
    "    return hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_mir_eval(logits, targets):\n",
    "    \"\"\"\n",
    "    logits:  (B, T, K)\n",
    "    targets: (B, T), bin labels, 0 = unvoiced\n",
    "    returns: dict with mir_eval metrics (averaged over batch)\n",
    "    \"\"\"\n",
    "    logits = logits.detach().cpu()\n",
    "    targets = targets.detach().cpu()\n",
    "\n",
    "    B = logits.shape[0]\n",
    "\n",
    "    metrics_sum = {\n",
    "        \"VRR\": 0.0,\n",
    "        \"RPA\": 0.0,\n",
    "        \"RCA\": 0.0,\n",
    "        \"OA\": 0.0,\n",
    "    }\n",
    "\n",
    "    for b in range(B):\n",
    "        pred_bins = torch.argmax(logits[b], dim=-1).numpy()\n",
    "        gt_bins   = targets[b].numpy()\n",
    "\n",
    "        pred_hz = bin_to_hz(pred_bins)\n",
    "        gt_hz   = bin_to_hz(gt_bins)\n",
    "\n",
    "        scores = mir_eval.melody.evaluate(\n",
    "            ref_time=np.arange(len(gt_hz)),\n",
    "            ref_freq=gt_hz,\n",
    "            est_time=np.arange(len(pred_hz)),\n",
    "            est_freq=pred_hz,\n",
    "        )\n",
    "\n",
    "        metrics_sum[\"VRR\"] += scores[\"Voicing Recall\"]\n",
    "        metrics_sum[\"RPA\"] += scores[\"Raw Pitch Accuracy\"]\n",
    "        metrics_sum[\"RCA\"] += scores[\"Raw Chroma Accuracy\"]\n",
    "        metrics_sum[\"OA\"]  += scores[\"Overall Accuracy\"]\n",
    "\n",
    "    for k in metrics_sum:\n",
    "        metrics_sum[k] /= B\n",
    "\n",
    "    return metrics_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_ce_loss(logits, targets, mask):\n",
    "    B, T, C = logits.shape\n",
    "\n",
    "    ce = F.cross_entropy(\n",
    "        logits.view(-1, C),\n",
    "        targets.view(-1),\n",
    "        reduction=\"none\"\n",
    "    ).view(B, T)\n",
    "\n",
    "    loss = (ce * mask).sum() / (mask.sum() + 1e-8)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal CE weighted loss\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X, Y, mask, _ in loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)              # (B, T, K)\n",
    "        loss = masked_ce_loss(logits, Y, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_hz(bin_idx):\n",
    "    \"\"\"\n",
    "    bin_idx: (T,) int, 0 = unvoiced\n",
    "    returns: (T,) float Hz\n",
    "    \"\"\"\n",
    "    bin_idx = np.asarray(bin_idx)\n",
    "    hz = np.zeros_like(bin_idx, dtype=np.float32)\n",
    "\n",
    "    voiced = bin_idx > 0\n",
    "    cents = C_MIN + bin_idx[voiced] * CENTS_PER_BIN\n",
    "    hz[voiced] = F_REF * (2.0 ** (cents / 1200.0))\n",
    "\n",
    "    return hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ea1ce-62ad-46fa-a83f-9d67653d8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "F_REF = 55.0\n",
    "C_MIN = -1200.0\n",
    "CENTS_PER_BIN = 20.0\n",
    "N_BINS = 360        # including bin 0 = unvoiced\n",
    "\n",
    "def hz_to_bin(f0_hz):\n",
    "    \"\"\"\n",
    "    f0_hz: (T,) array-like, Hz, 0 = unvoiced\n",
    "    returns: (T,) int bins, 0 = unvoiced\n",
    "    \"\"\"\n",
    "    f0_hz = np.asarray(f0_hz)\n",
    "    bins = np.zeros_like(f0_hz, dtype=np.int64)\n",
    "\n",
    "    voiced = f0_hz > 0\n",
    "    cents = 1200.0 * np.log2(f0_hz[voiced] / F_REF)\n",
    "\n",
    "    bins_voiced = np.floor((cents - C_MIN) / CENTS_PER_BIN).astype(np.int64)\n",
    "    bins_voiced = np.clip(bins_voiced, 1, N_BINS - 1)\n",
    "\n",
    "    bins[voiced] = bins_voiced\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voiced_mask_from_target(y):\n",
    "    return y > 0   # bin 0 = unvoiced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d945ac8-0cff-4f4e-8cc8-b85807432abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_loss(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y, mask, _ in loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            logits = model(X)                 # (B, T, K)\n",
    "            loss = masked_ce_loss(logits, Y, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n += 1\n",
    "\n",
    "    return total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a741d-703b-4c2a-8087-dacf3820b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mir_eval\n",
    "\n",
    "def evaluate_full(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    metrics_sum = {\n",
    "        \"VRR\": 0.0,\n",
    "        \"RPA\": 0.0,\n",
    "        \"RCA\": 0.0,\n",
    "        \"OA\":  0.0,\n",
    "    }\n",
    "    n_seq = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y, _, lengths in loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            logits = model(X)  # (B, T, K)\n",
    "            preds = torch.argmax(logits, dim=-1)  # (B, T)\n",
    "\n",
    "            B = X.shape[0]\n",
    "\n",
    "            for b in range(B):\n",
    "                T = lengths[b].item()\n",
    "\n",
    "                gt_bins   = Y[b, :T].cpu().numpy()\n",
    "                pred_bins = preds[b, :T].cpu().numpy()\n",
    "\n",
    "                gt_hz   = bin_to_hz(gt_bins)\n",
    "                pred_hz = bin_to_hz(pred_bins)\n",
    "\n",
    "                # dummy time axis (mir_eval only needs alignment)\n",
    "                t = np.arange(T)\n",
    "\n",
    "                scores = mir_eval.melody.evaluate(\n",
    "                    ref_time=t,\n",
    "                    ref_freq=gt_hz,\n",
    "                    est_time=t,\n",
    "                    est_freq=pred_hz,\n",
    "                )\n",
    "\n",
    "                metrics_sum[\"VRR\"] += scores[\"Voicing Recall\"]\n",
    "                metrics_sum[\"RPA\"] += scores[\"Raw Pitch Accuracy\"]\n",
    "                metrics_sum[\"RCA\"] += scores[\"Raw Chroma Accuracy\"]\n",
    "                metrics_sum[\"OA\"]  += scores[\"Overall Accuracy\"]\n",
    "\n",
    "                n_seq += 1\n",
    "\n",
    "    for k in metrics_sum:\n",
    "        metrics_sum[k] /= max(n_seq, 1)\n",
    "\n",
    "    return metrics_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055476e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, num_layers):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_features, hidden_features, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)[0]\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_features, hidden_features, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lstm(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3159bc-062e-4dfc-b8bf-b1546a03c7c3",
   "metadata": {},
   "source": [
    "### Run from here for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e660e-cc22-4329-8032-963f2214737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1b5e7-ae71-4db5-8fc1-86f67de1f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del train_loader\n",
    "del val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978f94-e633-416e-b5d4-02b3eb58b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66577d-0e97-4124-bfe0-837691644008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c653825-e1b8-4caa-b09a-69905a56a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "rrcgd_dir = \"dataset/whole_data/train/rrcgd\"\n",
    "label_dir = \"dataset/whole_data/train/labels\"\n",
    "\n",
    "dataset = RRCGDDataset(rrcgd_dir, label_dir)\n",
    "\n",
    "train_ds = RRCGDDataset(\n",
    "    \"dataset/whole_data/train/rrcgd\",\n",
    "    \"dataset/whole_data/train/labels\"\n",
    ")\n",
    "val_ds = RRCGDDataset(\n",
    "    \"dataset/whole_data/val/rrcgd\",\n",
    "    \"dataset/whole_data/val/labels\"\n",
    ")\n",
    "test_ds = RRCGDDataset(\n",
    "    \"dataset/whole_data/test/rrcgd\",\n",
    "    \"dataset/whole_data/test/labels\"\n",
    ")\n",
    "batch_size=16\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TemporalResBlock(nn.Module):\n",
    "#     def __init__(self, ch, dilation):\n",
    "#         super().__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.Conv1d(ch, ch, 3, padding=dilation, dilation=dilation),\n",
    "#             nn.BatchNorm1d(ch),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(ch, ch, 3, padding=dilation, dilation=dilation),\n",
    "#             nn.BatchNorm1d(ch),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.relu(x + self.block(x))\n",
    "\n",
    "\n",
    "# class RRCGDNet(nn.Module):\n",
    "#     def __init__(self, n_bins=360):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.frontend = nn.Sequential(\n",
    "#             nn.Conv1d(256, 128, 3, padding=1),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             TemporalResBlock(128, dilation=2),\n",
    "#             TemporalResBlock(128, dilation=4),\n",
    "#         )\n",
    "\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=128,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.2\n",
    "#         )\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(384, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, n_bins)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B,T,256)\n",
    "#         x = x.transpose(1, 2)      # (B,256,T)\n",
    "#         x = self.frontend(x)\n",
    "#         x = x.transpose(1, 2)      # (B,T,128)\n",
    "#         x, _ = self.gru(x)         # (B,T,384)\n",
    "#         return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8d9bd-882c-4149-9c12-199f5bf0b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be17e6a-cddd-4db8-b3a4-89920aecd583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9102c-c483-4ffb-879c-f1352772c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.conv = ConvBlock(in_ch, out_ch)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=(2,1))  # ↓ time only\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         return x, self.pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba691d8-81dd-427a-a55d-10f1587c2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.up = nn.ConvTranspose2d(\n",
    "#             in_ch, out_ch, kernel_size=(2,1), stride=(2,1)\n",
    "#         )\n",
    "#         self.conv = ConvBlock(in_ch, out_ch)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.up(x)\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ec66b-2f19-4783-8da2-bce96806f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleUNet(nn.Module):\n",
    "#     def __init__(self, n_bins=360):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.enc1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, 3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.enc2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, 3, stride=(2,1), padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Bottleneck\n",
    "#         self.mid = nn.Sequential(\n",
    "#             nn.Conv2d(128, 256, 3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Decoder (NOTE channel fix)\n",
    "#         self.dec1 = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(256, 128, kernel_size=(2,1), stride=(2,1)),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.out = nn.Conv2d(128, 1, 1)\n",
    "\n",
    "#         # Temporal modeling\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=256,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True\n",
    "#         )\n",
    "\n",
    "#         self.fc = nn.Linear(384, n_bins)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B,T,256)\n",
    "#         x = x.unsqueeze(1).transpose(2,3)   # (B,1,256,T)\n",
    "\n",
    "#         e1 = self.enc1(x)                    # (B,64,256,T)\n",
    "#         e2 = self.enc2(e1)                   # (B,128,128,T)\n",
    "#         m  = self.mid(e2)                    # (B,256,128,T)\n",
    "\n",
    "#         d1 = self.dec1(m)                    # (B,128,256,T)\n",
    "#         o  = self.out(d1).squeeze(1)         # (B,256,T)\n",
    "\n",
    "#         o = o.transpose(1,2)                 # (B,T,256)\n",
    "#         o,_ = self.gru(o)\n",
    "#         return self.fc(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6b7bf-0511-478c-87d2-6e763c19e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(cin, cout, 3, padding=1),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(cout, cout, 3, padding=1),\n",
    "            nn.BatchNorm2d(cout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edceb87f-0671-4e73-aa67-7bb6cda0f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "        self.pool = nn.AvgPool2d((2,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.conv(x)\n",
    "        return s, self.pool(s)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=(2,1), stride=(2,1))\n",
    "        self.conv = ConvBlock(out_ch + skip_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b226f7e-f950-48ed-9a4a-bc6e59c32f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Model 1 \n",
    "\n",
    "# class RRCGDNet_UNet(nn.Module):\n",
    "#     def __init__(self, n_bins=300):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Encoder (shallower)\n",
    "#         self.down1 = Down(1, 64)\n",
    "#         self.down2 = Down(64, 128)\n",
    "#         self.down3 = Down(128, 256)\n",
    "\n",
    "#         # Bottleneck (256 → 256)\n",
    "#         self.mid = ConvBlock(256, 256)\n",
    "\n",
    "#         # Decoder\n",
    "#         self.up3 = Up(256, 256, 128)\n",
    "#         self.up2 = Up(128, 128, 64)\n",
    "#         self.up1 = Up(64, 64, 64)\n",
    "\n",
    "#         self.out_conv = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "#         # Channel expansion\n",
    "#         self.expand = nn.Conv1d(256, 512, 1)\n",
    "\n",
    "#         # Temporal frontend\n",
    "#         self.temporal = nn.Sequential(\n",
    "#             nn.Conv1d(512, 512, 3, padding=1),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(512, 512, 3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         # BiGRU\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=512,\n",
    "#             hidden_size=192,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.2\n",
    "#         )\n",
    "\n",
    "#         self.fc = nn.Linear(384, 360)\n",
    "#         self.act = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B, T, 256)\n",
    "#         x = x.unsqueeze(1).transpose(2, 3)  # (B,1,256,T)\n",
    "\n",
    "#         s1, x = self.down1(x)\n",
    "#         s2, x = self.down2(x)\n",
    "#         s3, x = self.down3(x)\n",
    "\n",
    "#         x = self.mid(x)\n",
    "\n",
    "#         x = self.up3(x, s3)\n",
    "#         x = self.up2(x, s2)\n",
    "#         x = self.up1(x, s1)\n",
    "\n",
    "#         x = self.out_conv(x).squeeze(1)   # (B,256,T)\n",
    "\n",
    "#         x = self.expand(x)                # (B,512,T)\n",
    "#         x = self.temporal(x)              # (B,512,T)\n",
    "\n",
    "#         x = x.transpose(1, 2)             # (B,T,512)\n",
    "\n",
    "#         x, _ = self.gru(x)\n",
    "#         x = self.fc(x)\n",
    "#         return self.act(x)   # (B, T, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d59af-0787-41e4-b79e-eb30d6d61745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2\n",
    "class RRCGDNet_UNet(nn.Module):\n",
    "    def __init__(self, n_bins=360):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = Down(1, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256,512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.mid = ConvBlock(512, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = Up(512,512,256)\n",
    "        self.up3 = Up(256, 256, 128)\n",
    "        self.up2 = Up(128, 128, 64)\n",
    "        self.up1 = Up(64, 64, 64)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "        self.expand = nn.Conv1d(256,512,1)\n",
    "\n",
    "        # Temporal frontend\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # BiGRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=512,\n",
    "            hidden_size=192,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(384, n_bins)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 256)\n",
    "        x = x.unsqueeze(1).transpose(2, 3)  # (B,1,256,T)\n",
    "\n",
    "        s1, x = self.down1(x)\n",
    "        s2, x = self.down2(x)\n",
    "        s3, x = self.down3(x)\n",
    "        s4, x = self.down4(x)\n",
    "\n",
    "        x = self.mid(x)\n",
    "\n",
    "        x = self.up4(x, s4)\n",
    "        x = self.up3(x, s3)\n",
    "        x = self.up2(x, s2)\n",
    "        x = self.up1(x, s1)\n",
    "\n",
    "        x = self.out_conv(x).squeeze(1)   # (B,256,T)\n",
    "        \n",
    "        # DO NOT transpose here\n",
    "        x = self.expand(x)                # (B,512,T)\n",
    "        x = self.temporal(x)              # (B,512,T)\n",
    "        \n",
    "        # Only now convert to (B,T,C) for GRU\n",
    "        x = x.transpose(1, 2)             # (B,T,512)\n",
    "        \n",
    "        x, _ = self.gru(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ab5d7-71a9-4ecc-8e56-5104eaed62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model3 \n",
    "\n",
    "# class RRCGDNet_UNet(nn.Module):\n",
    "#     def __init__(self, n_bins=360):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.down1 = Down(1, 64)\n",
    "#         self.down2 = Down(64, 128)\n",
    "#         self.down3 = Down(128, 256)\n",
    "#         self.down4 = Down(256,512)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         self.mid = ConvBlock(512, 512)\n",
    "\n",
    "#         # Decoder\n",
    "#         self.up4 = Up(512,512,256)\n",
    "#         self.up3 = Up(256, 256, 128)\n",
    "#         self.up2 = Up(128, 128, 64)\n",
    "#         self.up1 = Up(64, 64, 64)\n",
    "\n",
    "#         self.out_conv = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "#         self.expand = nn.Conv1d(256,512,1)\n",
    "\n",
    "#         # Temporal frontend\n",
    "#         # ---- Channel expansion ----\n",
    "#         self.expand1 = nn.Conv1d(256, 512, kernel_size=1)\n",
    "#         self.expand2 = nn.Conv1d(512, 1024, kernel_size=1)\n",
    "        \n",
    "#         # ---- Temporal blocks ----\n",
    "#         self.temporal1 = nn.Sequential(\n",
    "#             nn.Conv1d(512, 512, 3, padding=1),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(512, 512, 3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        \n",
    "#         self.temporal2 = nn.Sequential(\n",
    "#             nn.Conv1d(1024, 1024, 3, padding=1),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(1024, 1024, 3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=1024,\n",
    "#             hidden_size=256,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.2\n",
    "#         )\n",
    "        \n",
    "#         self.fc = nn.Linear(512, n_bins)\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B, T, 256)\n",
    "#         x = x.unsqueeze(1).transpose(2, 3)  # (B,1,256,T)\n",
    "\n",
    "#         s1, x = self.down1(x)\n",
    "#         s2, x = self.down2(x)\n",
    "#         s3, x = self.down3(x)\n",
    "#         s4, x = self.down4(x)\n",
    "\n",
    "#         x = self.mid(x)\n",
    "\n",
    "#         x = self.up4(x, s4)\n",
    "#         x = self.up3(x, s3)\n",
    "#         x = self.up2(x, s2)\n",
    "#         x = self.up1(x, s1)\n",
    "\n",
    "#         x = self.out_conv(x).squeeze(1)   # (B,256,T)\n",
    "        \n",
    "#         x = self.expand1(x)               # (B,512,T)\n",
    "#         x = self.temporal1(x)             # (B,512,T)\n",
    "        \n",
    "#         x = self.expand2(x)               # (B,1024,T)\n",
    "#         x = self.temporal2(x)             # (B,1024,T)\n",
    "        \n",
    "#         x = x.transpose(1, 2)             # (B,T,1024)\n",
    "        \n",
    "#         x, _ = self.gru(x)\n",
    "#         return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a3fdc-d0be-4a4e-8af5-c009ce2b76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model4-toeplitz FC layer \n",
    "# class ToeplitzPitchHead(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Toeplitz-structured pitch classification head.\n",
    "#     Enforces pitch-shift equivariance across bins.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, in_dim: int, n_bins: int):\n",
    "#         super().__init__()\n",
    "#         self.in_dim = in_dim\n",
    "#         self.n_bins = n_bins\n",
    "\n",
    "#         # Toeplitz kernel: length = in_dim + n_bins - 1\n",
    "#         self.weight = nn.Parameter(torch.randn(in_dim + n_bins - 1))\n",
    "#         self.bias = nn.Parameter(torch.zeros(n_bins))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (B, T, in_dim)\n",
    "#         returns: (B, T, n_bins)\n",
    "#         \"\"\"\n",
    "#         B, T, D = x.shape\n",
    "#         assert D == self.in_dim\n",
    "\n",
    "#         # Construct Toeplitz weight matrix efficiently\n",
    "#         # Shape: (n_bins, in_dim)\n",
    "#         W = self.weight.unfold(0, D, 1)\n",
    "\n",
    "#         # Compute logits\n",
    "#         logits = torch.einsum(\"btd,kd->btk\", x, W)\n",
    "#         logits = logits + self.bias\n",
    "\n",
    "#         return logits\n",
    "\n",
    "# class Toeplitz_FC_RRCGDNet_UNet(nn.Module):\n",
    "#     def __init__(self, n_bins=360):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # ---------------- Encoder ----------------\n",
    "#         self.down1 = Down(1, 64)\n",
    "#         self.down2 = Down(64, 128)\n",
    "#         self.down3 = Down(128, 256)\n",
    "#         self.down4 = Down(256, 512)\n",
    "\n",
    "#         # ---------------- Bottleneck ----------------\n",
    "#         self.mid = ConvBlock(512, 512)\n",
    "\n",
    "#         # ---------------- Decoder ----------------\n",
    "#         self.up4 = Up(512, 512, 256)\n",
    "#         self.up3 = Up(256, 256, 128)\n",
    "#         self.up2 = Up(128, 128, 64)\n",
    "#         self.up1 = Up(64, 64, 64)\n",
    "\n",
    "#         # Collapse UNet channels\n",
    "#         self.out_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "#         # ---------------- Temporal Frontend ----------------\n",
    "#         self.expand1 = nn.Conv1d(256, 512, kernel_size=1)\n",
    "\n",
    "#         self.temporal1 = nn.Sequential(\n",
    "#             nn.Conv1d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         self.expand2 = nn.Conv1d(512, 1024, kernel_size=1)\n",
    "\n",
    "#         self.temporal2 = nn.Sequential(\n",
    "#             nn.Conv1d(1024, 1024, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(1024, 1024, kernel_size=3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         # ---------------- Sequence Model ----------------\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=1024,\n",
    "#             hidden_size=256,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.2,\n",
    "#         )\n",
    "\n",
    "#         # ---------------- Toeplitz Pitch Head ----------------\n",
    "#         self.fc = ToeplitzPitchHead(in_dim=512, n_bins=n_bins)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (B, T, 256)  — RRCGD features\n",
    "#         returns: (B, T, n_bins)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # (B, 1, 256, T)\n",
    "#         x = x.unsqueeze(1).transpose(2, 3)\n",
    "\n",
    "#         # Encoder\n",
    "#         s1, x = self.down1(x)\n",
    "#         s2, x = self.down2(x)\n",
    "#         s3, x = self.down3(x)\n",
    "#         s4, x = self.down4(x)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         x = self.mid(x)\n",
    "\n",
    "#         # Decoder\n",
    "#         x = self.up4(x, s4)\n",
    "#         x = self.up3(x, s3)\n",
    "#         x = self.up2(x, s2)\n",
    "#         x = self.up1(x, s1)\n",
    "\n",
    "#         # Collapse channels → (B, 256, T)\n",
    "#         x = self.out_conv(x).squeeze(1)\n",
    "\n",
    "#         # Temporal frontend\n",
    "#         x = self.expand1(x)        # (B, 512, T)\n",
    "#         x = self.temporal1(x)\n",
    "\n",
    "#         x = self.expand2(x)        # (B, 1024, T)\n",
    "#         x = self.temporal2(x)\n",
    "\n",
    "#         # Sequence modeling\n",
    "#         x = x.transpose(1, 2)      # (B, T, 1024)\n",
    "#         x, _ = self.gru(x)         # (B, T, 512)\n",
    "\n",
    "#         # Toeplitz FC pitch head\n",
    "#         logits = self.fc(x)        # (B, T, n_bins)\n",
    "\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f6bf1-b0ac-4be6-82d5-3df7169ee4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# x = torch.randn(2, 100, 256).to(device)\n",
    "# model = SimpleUNet().to(device)\n",
    "# y = model(x)\n",
    "# print(y.shape)  # must be (2, 100, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d68495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61f922-c07a-4962-b160-6bb6750b539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRCGDNet_UNet based model definition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RRCGDNet_UNet().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=2e-5,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01a363-a66e-4cf2-a0a7-9f5b39ec3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x = torch.randn(1, 200, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f4b7d-bb9d-4391-900c-cb146f987377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, FREQ = 2, 100, 256\n",
    "# N_BINS = 360\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Toeplitz_FC_RRCGDNet_UNet(n_bins=N_BINS)\n",
    "# x = torch.randn(B, T, FREQ)\n",
    "# y = model(x)\n",
    "# print(y.shape)  # must be (2, 100, 360)\n",
    "\n",
    "# y.mean().backward()\n",
    "# assert model.fc.weight.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f2b7a-334b-4003-b678-43c55349f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),lr=2e-5,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a1b7a-cf56-490b-8457-b3aae858ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e869ed-2f85-44c2-be32-3ec58c8037a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8016c84-f676-4755-bbbd-55fb9849fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping based on metric\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, min_delta=1e-3):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = 0.0\n",
    "        self.count = 0\n",
    "        self.stop = False\n",
    "\n",
    "    def step(self, rpa):\n",
    "        # Improvement only if RPA increases meaningfully\n",
    "        if rpa > self.best + self.min_delta:\n",
    "            self.best = rpa\n",
    "            self.count = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                self.stop = True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23daab1c-f020-4615-9355-b314b3409032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping based on validation loss-much better\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = float(\"inf\")   # LOSS is minimized\n",
    "        self.count = 0\n",
    "        self.stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        # Improvement = sufficient decrease in loss\n",
    "        if val_loss < self.best - self.min_delta:\n",
    "            self.best = val_loss\n",
    "            self.count = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                self.stop = True\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136f978-4897-4fd4-ad4f-0064a4ce26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, best_val_oa, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    model_state = (\n",
    "        model.module.state_dict()\n",
    "        if isinstance(model, torch.nn.DataParallel)\n",
    "        else model.state_dict()\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model_state,\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_oa\": best_val_oa,\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, device):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model.module.load_state_dict(ckpt[\"model_state\"])\n",
    "    else:\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_val_oa = ckpt[\"best_val_oa\"]\n",
    "\n",
    "    return start_epoch, best_val_oa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663e20a-c2da-4d03-9b46-5cad924057e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def log_epoch_metrics_txt(\n",
    "    filepath,\n",
    "    epoch,\n",
    "    train_loss,\n",
    "    val_loss,\n",
    "    vrr,\n",
    "    rpa,\n",
    "    oa,\n",
    "    mode=\"a\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs one epoch of training/validation metrics to a text file.\n",
    "\n",
    "    filepath : str\n",
    "        Path to .txt log file\n",
    "    epoch : int\n",
    "    train_loss : float\n",
    "    val_loss : float\n",
    "    vrr : float\n",
    "    rpa : float\n",
    "    oa : float\n",
    "    mode : str\n",
    "        \"a\" = append (default), \"w\" = overwrite\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    with open(filepath, mode) as f:\n",
    "        # write header if file is new or overwritten\n",
    "        if mode == \"w\":\n",
    "            f.write(\n",
    "                \"Epoch\\tTrainLoss\\tValLoss\\tVRR\\tRPA\\tOA\\n\"\n",
    "            )\n",
    "\n",
    "        f.write(\n",
    "            f\"{epoch:03d}\\t\"\n",
    "            f\"{train_loss:.6f}\\t\"\n",
    "            f\"{val_loss:.6f}\\t\"\n",
    "            f\"{vrr:.4f}\\t\"\n",
    "            f\"{rpa:.4f}\\t\"\n",
    "            f\"{oa:.4f}\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00612c-ea5e-4b7f-acb8-b0f065abab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/train_metrics_log.txt\"\n",
    "\n",
    "# overwrite old log at start of run\n",
    "log_epoch_metrics_txt(\n",
    "    log_path,\n",
    "    epoch=0,\n",
    "    train_loss=0,\n",
    "    val_loss=0,\n",
    "    vrr=0,\n",
    "    rpa=0,\n",
    "    oa=0,\n",
    "    mode=\"w\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa54223-af86-425a-a07c-be59b86eb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists(log_path))  # must print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8c3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_vrrs, val_rpas, val_oas = [], [], []\n",
    "\n",
    "best_val_oa = 0.0\n",
    "start_epoch = 1\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "LAST_PATH=\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/last.pt\"\n",
    "# ---- Resume if exists ----\n",
    "if os.path.exists(LAST_PATH):\n",
    "    start_epoch, best_val_oa = load_checkpoint(\n",
    "        LAST_PATH, model, optimizer, device\n",
    "    )\n",
    "\n",
    "# ---- Training loop ----\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "\n",
    "    # ---- Train ----\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, device\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ---- Validate (loss) ----\n",
    "    val_loss = validate_with_loss(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # ---- Validate (mir_eval metrics) ----\n",
    "    metrics = evaluate_full(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "    vrr = metrics[\"VRR\"]\n",
    "    rpa = metrics[\"RPA\"]\n",
    "    oa  = metrics[\"OA\"]\n",
    "\n",
    "    val_vrrs.append(vrr)\n",
    "    val_rpas.append(rpa)\n",
    "    val_oas.append(oa)\n",
    "\n",
    "    # ---- Scheduler (OA-driven) ----\n",
    "    scheduler.step(oa)\n",
    "\n",
    "    # ---- Save last checkpoint ----\n",
    "    save_checkpoint(\n",
    "        epoch, model, optimizer, best_val_oa,\n",
    "        \"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/last.pt\"\n",
    "    )\n",
    "\n",
    "    # ---- Save best model (OA only) ----\n",
    "    if oa > best_val_oa:\n",
    "        best_val_oa = oa\n",
    "        save_checkpoint(\n",
    "            epoch, model, optimizer, best_val_oa,\n",
    "            \"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/best_oa.pt\"\n",
    "        )\n",
    "\n",
    "    # ---- Logging ----\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"Train {train_loss:.6f} | \"\n",
    "        f\"Val {val_loss:.6f} | \"\n",
    "        f\"VRR {vrr:.3f} | RPA {rpa:.3f} | OA {oa:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- Early stopping (same signal as scheduler) ----\n",
    "    early_stopper.step(oa)\n",
    "    if early_stopper.stop:\n",
    "        print(\"Early stopping happened!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b742ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# ---- Loss ----\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ---- Metrics ----\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, val_vrrs, label=\"VRR\")\n",
    "plt.plot(epochs, val_rpas, label=\"RPA\")\n",
    "plt.plot(epochs, val_oas, label=\"OA\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Metrics\")\n",
    "plt.ylim(0,1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/training_curves.png\", dpi=200)  # SAVE FIRST\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddcfb0-76ab-4847-be23-ab8a4a5a7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ema(x, alpha=0.3):\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    y[0] = x[0]\n",
    "    for i in range(1, len(x)):\n",
    "        y[i] = alpha * x[i] + (1 - alpha) * y[i-1]\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9a5bc-5c59-438e-8b8f-ac8f499374d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_s = ema(train_losses, alpha=0.3)\n",
    "val_losses_s   = ema(val_losses, alpha=0.3)\n",
    "\n",
    "val_vrrs_s = ema(val_vrrs, alpha=0.3)\n",
    "val_rpas_s = ema(val_rpas, alpha=0.3)\n",
    "val_oas_s  = ema(val_oas,  alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08461ca-f13f-4a29-898d-08517b8ec992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87137e61-7a64-44f6-ad0a-b73f58e18e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "# ---- Loss ----\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(epochs, ema(train_losses, 0.3), label=\"Train Loss\")\n",
    "plt.plot(epochs, ema(val_losses, 0.3), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss (EMA)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ---- Metrics ----\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(epochs, ema(val_vrrs, 0.3), label=\"VRR\")\n",
    "plt.plot(epochs, ema(val_rpas, 0.3), label=\"RPA\")\n",
    "plt.plot(epochs, ema(val_oas,  0.3), label=\"OA\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title(\"Validation Metrics (EMA)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/smooth_training_curves.png\", dpi=200)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_full(model, test_loader, device)\n",
    "\n",
    "test_vrr = metrics[\"VRR\"]\n",
    "test_rpa = metrics[\"RPA\"]\n",
    "test_oa  = metrics[\"OA\"]\n",
    "\n",
    "print(\n",
    "    f\"[TEST] VRR={test_vrr:.3f} | \"\n",
    "    f\"RPA={test_rpa:.3f} | OA={test_oa:.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63d32f-0e95-42a1-acf8-76cf10970a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            total += p.numel()\n",
    "    return total\n",
    "\n",
    "n_params = model_summary(model)\n",
    "print(f\"Total trainable parameters: {n_params:,}\")\n",
    "\n",
    "with open(\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/model_stats.txt\", \"w\") as f:\n",
    "    f.write(f\"Total trainable parameters: {n_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c362ec-799c-48d6-93c7-cad5e630f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_test_results(\n",
    "    test_vrr, test_rpa, test_oa,\n",
    "    ckpt_dir=\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/\"\n",
    "):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    results = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"VRR\": test_vrr,\n",
    "        \"RPA\": test_rpa,\n",
    "        \"OA\": test_oa,\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(ckpt_dir, \"test_results.json\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"[LOGGED] Test results → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25841b3-30bb-4c00-891c-c9f0fed426ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_results(test_vrr, test_rpa, test_oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd2f6e-922f-4ada-87c0-a6ec8ba5444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "export_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "export_model = export_model.to(\"cpu\")   # safer for scripting\n",
    "\n",
    "scripted = torch.jit.script(export_model)\n",
    "scripted.save(\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/model_torchscript.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181a9b0-6228-4bfd-b0cf-66d79a797f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfd6ed-7e1b-454c-a135-feb7d4d97725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    export_model,\n",
    "    input_size=(1, 200, 256),  # (B,T,F)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88884386-b867-4d72-9fba-c06fc68a8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa//summary.txt\", \"w\") as f:\n",
    "    f.write(str(summary(export_model, input_size=(1,200,256))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd23de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_voicing(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    TP = FP = FN = TN = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y, _, lengths in loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            B = X.shape[0]\n",
    "\n",
    "            for b in range(B):\n",
    "                T = lengths[b].item()\n",
    "\n",
    "                gt_bins   = Y[b, :T].cpu().numpy()\n",
    "                pred_bins = preds[b, :T].cpu().numpy()\n",
    "\n",
    "                gt_voiced   = gt_bins > 0\n",
    "                pred_voiced = pred_bins > 0\n",
    "\n",
    "                TP += np.sum(pred_voiced & gt_voiced)\n",
    "                FP += np.sum(pred_voiced & ~gt_voiced)\n",
    "                FN += np.sum(~pred_voiced & gt_voiced)\n",
    "                TN += np.sum(~pred_voiced & ~gt_voiced)\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall    = TP / (TP + FN + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TN\": TN,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_voicing_confusion_matrix(cm, normalize=True):\n",
    "    \"\"\"\n",
    "    cm: 2x2 confusion matrix\n",
    "    normalize: show percentages instead of counts\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm / (cm.sum(axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    im = ax.imshow(cm)\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([\"Unvoiced\", \"Voiced\"])\n",
    "    ax.set_yticklabels([\"Unvoiced\", \"Voiced\"])\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Ground Truth\")\n",
    "    ax.set_title(\"Voicing Confusion Matrix\")\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = cm[i, j]\n",
    "            txt = f\"{val:.2f}\" if normalize else f\"{int(val)}\"\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3002ffe-23df-46c6-b47b-d3843c751965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def voicing_confusion_matrix(voicing_metrics):\n",
    "    TP = voicing_metrics[\"TP\"]\n",
    "    FP = voicing_metrics[\"FP\"]\n",
    "    FN = voicing_metrics[\"FN\"]\n",
    "    TN = voicing_metrics[\"TN\"]\n",
    "\n",
    "    # rows = ground truth, cols = prediction\n",
    "    return np.array([\n",
    "        [TN, FP],\n",
    "        [FN, TP]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ec413",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicing_metrics = evaluate_voicing(model, test_loader, device)\n",
    "\n",
    "print(\n",
    "    f\"[VOICING] \"\n",
    "    f\"Precision={voicing_metrics['Precision']:.3f} | \"\n",
    "    f\"Recall={voicing_metrics['Recall']:.3f} | \"\n",
    "    f\"F1={voicing_metrics['F1']:.3f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Confusion: \"\n",
    "    f\"TP={voicing_metrics['TP']} | \"\n",
    "    f\"FP={voicing_metrics['FP']} | \"\n",
    "    f\"FN={voicing_metrics['FN']} | \"\n",
    "    f\"TN={voicing_metrics['TN']}\"\n",
    ")\n",
    "\n",
    "cm = voicing_confusion_matrix(voicing_metrics)\n",
    "\n",
    "plot_voicing_confusion_matrix(cm, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d7379-464d-4b68-bdcf-51f7555ec9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voicing_scores(voicing_metrics):\n",
    "    labels = [\"Precision\", \"Recall\", \"F1\"]\n",
    "    values = [\n",
    "        voicing_metrics[\"Precision\"],\n",
    "        voicing_metrics[\"Recall\"],\n",
    "        voicing_metrics[\"F1\"],\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Voicing Metrics\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee5f52-5625-4cc4-9ea8-277e55d3ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voicing_scores(voicing_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edd767-a219-4425-8164-0f3bd5acdf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_voicing_metrics(voicing_metrics, out_path):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(voicing_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c2968-a1d2-48e1-b693-e9203be20c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_voicing_metrics_txt(voicing_metrics, out_path):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"Voicing metrics\\n\")\n",
    "        f.write(\"================\\n\")\n",
    "        f.write(f\"Precision : {float(voicing_metrics['Precision']):.6f}\\n\")\n",
    "        f.write(f\"Recall    : {float(voicing_metrics['Recall']):.6f}\\n\")\n",
    "        f.write(f\"F1-score  : {float(voicing_metrics['F1']):.6f}\\n\")\n",
    "        f.write(\"\\nConfusion matrix counts\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        f.write(f\"TP : {int(voicing_metrics['TP'])}\\n\")\n",
    "        f.write(f\"FP : {int(voicing_metrics['FP'])}\\n\")\n",
    "        f.write(f\"FN : {int(voicing_metrics['FN'])}\\n\")\n",
    "        f.write(f\"TN : {int(voicing_metrics['TN'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d17f1d-261a-408f-ac64-cc1d94e23a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicing_metrics = evaluate_voicing(model, test_loader, device)\n",
    "\n",
    "save_voicing_metrics_txt(\n",
    "    voicing_metrics,\n",
    "    \"checkpoints/model_RRCGDNet_UNet2/CE_loss_best_oa/voicing_metrics_test.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81b305-ffa5-4aa3-aebe-d4d4253288f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gd_exps)",
   "language": "python",
   "name": "gd_exps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
