{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eebfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RRCGDDataset(Dataset):\n",
    "    def __init__(self, rrcgd_dir, label_dir):\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(rrcgd_dir)\n",
    "            if f.endswith(\"_rrcgd.npy\")\n",
    "        ])\n",
    "        self.rrcgd_dir = rrcgd_dir\n",
    "        self.label_dir = label_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rrcgd_path = os.path.join(self.rrcgd_dir, self.files[idx])\n",
    "        label_path = os.path.join(\n",
    "            self.label_dir,\n",
    "            self.files[idx].replace(\"_rrcgd.npy\", \".npy\")\n",
    "        )\n",
    "\n",
    "        X = np.load(rrcgd_path)              # (T, 256)\n",
    "        y_hz = np.load(label_path)        # (T,)\n",
    "\n",
    "        y = hz_to_bin(y_hz)                  # (T,)\n",
    "        X = torch.from_numpy(X).float()        # (T, 256)\n",
    "        y = torch.from_numpy(y).long()         # (T,)\n",
    "        \n",
    "        mask = (y > 0).float()                 # voiced mask\n",
    "        length = X.shape[0]                    # valid length\n",
    "        \n",
    "        return X, y, mask, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fe0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs, Ys, masks, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    Xs = pad_sequence(Xs, batch_first=True)      # (B, T, 256)\n",
    "    Ys = pad_sequence(Ys, batch_first=True)      # (B, T)\n",
    "    mask = pad_sequence(masks, batch_first=True) # (B, T)\n",
    "\n",
    "    return Xs, Ys, mask, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec115c68-78a4-4f20-ba1a-e71b54bdee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import mir_eval\n",
    "\n",
    "F_REF = 55.0\n",
    "C_MIN = -1200.0\n",
    "CENTS_PER_BIN = 20.0\n",
    "\n",
    "def bin_to_hz(bins):\n",
    "    if isinstance(bins, torch.Tensor):\n",
    "        bins = bins.detach().cpu().numpy()\n",
    "\n",
    "    hz = np.zeros_like(bins, dtype=np.float32)\n",
    "    voiced = bins > 0\n",
    "\n",
    "    cents = C_MIN + bins[voiced] * CENTS_PER_BIN\n",
    "    hz[voiced] = F_REF * (2.0 ** (cents / 1200.0))\n",
    "    return hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe07051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_mir_eval(logits, targets, lengths):\n",
    "    logits = logits.detach().cpu()\n",
    "    targets = targets.detach().cpu()\n",
    "\n",
    "    B = logits.shape[0]\n",
    "\n",
    "    metrics_sum = {\n",
    "        \"VRR\": 0.0,\n",
    "        \"RPA\": 0.0,\n",
    "        \"RCA\": 0.0,\n",
    "        \"OA\": 0.0,\n",
    "    }\n",
    "\n",
    "    for b in range(B):\n",
    "        T = lengths[b].item()\n",
    "\n",
    "        pred_bins = torch.argmax(logits[b, :T], dim=-1).numpy()\n",
    "        gt_bins   = targets[b, :T].numpy()\n",
    "\n",
    "        pred_hz = bin_to_hz(pred_bins)\n",
    "        gt_hz   = bin_to_hz(gt_bins)\n",
    "\n",
    "        t = np.arange(T)\n",
    "\n",
    "        scores = mir_eval.melody.evaluate(\n",
    "            ref_time=t,\n",
    "            ref_freq=gt_hz,\n",
    "            est_time=t,\n",
    "            est_freq=pred_hz,\n",
    "        )\n",
    "\n",
    "        metrics_sum[\"VRR\"] += scores[\"Voicing Recall\"]\n",
    "        metrics_sum[\"RPA\"] += scores[\"Raw Pitch Accuracy\"]\n",
    "        metrics_sum[\"RCA\"] += scores[\"Raw Chroma Accuracy\"]\n",
    "        metrics_sum[\"OA\"]  += scores[\"Overall Accuracy\"]\n",
    "\n",
    "    for k in metrics_sum:\n",
    "        metrics_sum[k] /= B\n",
    "\n",
    "    return metrics_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_ce_loss(logits, targets, mask):\n",
    "#     B, T, C = logits.shape\n",
    "\n",
    "#     ce = F.cross_entropy(\n",
    "#         logits.view(-1, C),\n",
    "#         targets.view(-1),\n",
    "#         reduction=\"none\"\n",
    "#     ).view(B, T)\n",
    "\n",
    "#     loss = (ce * mask).sum() / (mask.sum() + 1e-8)\n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa4f5dd-ff93-49a6-90ba-798b08571bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_smoothed_ce_loss(\n",
    "    logits,\n",
    "    targets,\n",
    "    mask,\n",
    "    sigma=1.5\n",
    "):\n",
    "    \"\"\"\n",
    "    logits:  (B, T, N_CLASS)  raw logits\n",
    "    targets: (B, T)           integer pitch bins\n",
    "    mask:    (B, T)           voiced / valid-frame mask\n",
    "    \"\"\"\n",
    "\n",
    "    B, T, C = logits.shape\n",
    "    device = logits.device\n",
    "\n",
    "    bins = torch.arange(C, device=device).float()  # (C,)\n",
    "\n",
    "    targets = targets.unsqueeze(-1).float()         # (B, T, 1)\n",
    "\n",
    "    smoothed = torch.exp(\n",
    "        -0.5 * ((bins - targets) ** 2) / (sigma ** 2)\n",
    "    )\n",
    "    smoothed = smoothed / (smoothed.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        logits,\n",
    "        smoothed,\n",
    "        reduction=\"none\"\n",
    "    )                                                # (B, T, C)\n",
    "\n",
    "    loss = loss.mean(dim=-1)                         # (B, T)\n",
    "    loss = (loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normal CE weighted loss\n",
    "# def train_one_epoch(model, loader, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for X, Y, mask, _ in loader:\n",
    "#         X = X.to(device)\n",
    "#         Y = Y.to(device)\n",
    "#         mask = mask.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(X)              # (B, T, K)\n",
    "#         loss = masked_ce_loss(logits, Y, mask)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f68f8e-5f13-4406-8eba-b23ee797984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch-smoothed CE weighted loss\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X, Y, mask, _ in loader:\n",
    "        X = X.to(device)       # (B, T, N_MELS)\n",
    "        Y = Y.to(device)       # (B, T) integer pitch bins\n",
    "        mask = mask.to(device) # (B, T) voiced mask {0,1}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)      # (B, T, N_CLASS) â€” logits\n",
    "        loss = pitch_smoothed_ce_loss(logits, Y, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddac3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_hz(bin_idx):\n",
    "    \"\"\"\n",
    "    bin_idx: (T,) int, 0 = unvoiced\n",
    "    returns: (T,) float Hz\n",
    "    \"\"\"\n",
    "    if isinstance(bin_idx, torch.Tensor):\n",
    "        bin_idx = bin_idx.detach().cpu().numpy()\n",
    "    else:\n",
    "        bin_idx = np.asarray(bin_idx)\n",
    "\n",
    "    hz = np.zeros_like(bin_idx, dtype=np.float32)\n",
    "\n",
    "    voiced = bin_idx > 0\n",
    "    cents = C_MIN + bin_idx[voiced] * CENTS_PER_BIN\n",
    "    hz[voiced] = F_REF * (2.0 ** (cents / 1200.0))\n",
    "\n",
    "    return hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ea1ce-62ad-46fa-a83f-9d67653d8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "F_REF = 55.0\n",
    "C_MIN = -1200.0\n",
    "CENTS_PER_BIN = 20.0\n",
    "N_BINS = 360        # including bin 0 = unvoiced\n",
    "\n",
    "def hz_to_bin(f0_hz):\n",
    "    \"\"\"\n",
    "    f0_hz: (T,) array-like, Hz, 0 = unvoiced\n",
    "    returns: (T,) int bins, 0 = unvoiced\n",
    "    \"\"\"\n",
    "    f0_hz = np.asarray(f0_hz)\n",
    "    bins = np.zeros_like(f0_hz, dtype=np.int64)\n",
    "\n",
    "    voiced = f0_hz > 0\n",
    "    cents = 1200.0 * np.log2(f0_hz[voiced] / F_REF)\n",
    "\n",
    "    bins_voiced = np.floor((cents - C_MIN) / CENTS_PER_BIN).astype(np.int64)\n",
    "    bins_voiced = np.clip(bins_voiced, 1, N_BINS - 1)\n",
    "\n",
    "    bins[voiced] = bins_voiced\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voiced_mask_from_target(y):\n",
    "    return y > 0   # bin 0 = unvoiced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d945ac8-0cff-4f4e-8cc8-b85807432abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_loss(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y, mask, _ in loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            logits = model(X)                 # (B, T, K)\n",
    "            loss = pitch_smoothed_ce_loss(logits, Y, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n += 1\n",
    "\n",
    "    return total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a741d-703b-4c2a-8087-dacf3820b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mir_eval\n",
    "\n",
    "def evaluate_full(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    metrics_sum = {\n",
    "        \"VRR\": 0.0,\n",
    "        \"RPA\": 0.0,\n",
    "        \"RCA\": 0.0,\n",
    "        \"OA\":  0.0,\n",
    "    }\n",
    "    n_seq = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y, _, lengths in loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            logits = model(X)  # (B, T, K)\n",
    "            preds = torch.argmax(logits, dim=-1)  # (B, T)\n",
    "\n",
    "            B = X.shape[0]\n",
    "\n",
    "            for b in range(B):\n",
    "                T = lengths[b].item()\n",
    "\n",
    "                gt_bins   = Y[b, :T].cpu().numpy()\n",
    "                pred_bins = preds[b, :T].cpu().numpy()\n",
    "\n",
    "                gt_hz   = bin_to_hz(gt_bins)\n",
    "                pred_hz = bin_to_hz(pred_bins)\n",
    "\n",
    "                # dummy time axis (mir_eval only needs alignment)\n",
    "                t = np.arange(T)\n",
    "\n",
    "                scores = mir_eval.melody.evaluate(\n",
    "                    ref_time=t,\n",
    "                    ref_freq=gt_hz,\n",
    "                    est_time=t,\n",
    "                    est_freq=pred_hz,\n",
    "                )\n",
    "\n",
    "                metrics_sum[\"VRR\"] += scores[\"Voicing Recall\"]\n",
    "                metrics_sum[\"RPA\"] += scores[\"Raw Pitch Accuracy\"]\n",
    "                metrics_sum[\"RCA\"] += scores[\"Raw Chroma Accuracy\"]\n",
    "                metrics_sum[\"OA\"]  += scores[\"Overall Accuracy\"]\n",
    "\n",
    "                n_seq += 1\n",
    "\n",
    "    for k in metrics_sum:\n",
    "        metrics_sum[k] /= max(n_seq, 1)\n",
    "\n",
    "    return metrics_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3159bc-062e-4dfc-b8bf-b1546a03c7c3",
   "metadata": {},
   "source": [
    "### Run from here for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e660e-cc22-4329-8032-963f2214737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1b5e7-ae71-4db5-8fc1-86f67de1f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del optimizer\n",
    "# del train_loader\n",
    "# del val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39978f94-e633-416e-b5d4-02b3eb58b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66577d-0e97-4124-bfe0-837691644008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c653825-e1b8-4caa-b09a-69905a56a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "rrcgd_dir = \"dataset/whole_data/train/rrcgd\"\n",
    "label_dir = \"dataset/whole_data/train/labels\"\n",
    "\n",
    "dataset = RRCGDDataset(rrcgd_dir, label_dir)\n",
    "\n",
    "train_ds = RRCGDDataset(\n",
    "    \"dataset/whole_data/train/rrcgd\",\n",
    "    \"dataset/whole_data/train/labels\"\n",
    ")\n",
    "val_ds = RRCGDDataset(\n",
    "    \"dataset/whole_data/val/rrcgd\",\n",
    "    \"dataset/whole_data/val/labels\"\n",
    ")\n",
    "test_ds = RRCGDDataset(\n",
    "    \"dataset/whole_data/test/rrcgd\",\n",
    "    \"dataset/whole_data/test/labels\"\n",
    ")\n",
    "batch_size=8\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a3fdc-d0be-4a4e-8af5-c009ce2b76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model4-toeplitz FC layer \n",
    "# class ToeplitzPitchHead(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Toeplitz-structured pitch classification head.\n",
    "#     Enforces pitch-shift equivariance across bins.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, in_dim: int, n_bins: int):\n",
    "#         super().__init__()\n",
    "#         self.in_dim = in_dim\n",
    "#         self.n_bins = n_bins\n",
    "\n",
    "#         # Toeplitz kernel: length = in_dim + n_bins - 1\n",
    "#         self.weight = nn.Parameter(torch.randn(in_dim + n_bins - 1))\n",
    "#         self.bias = nn.Parameter(torch.zeros(n_bins))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (B, T, in_dim)\n",
    "#         returns: (B, T, n_bins)\n",
    "#         \"\"\"\n",
    "#         B, T, D = x.shape\n",
    "#         assert D == self.in_dim\n",
    "\n",
    "#         # Construct Toeplitz weight matrix efficiently\n",
    "#         # Shape: (n_bins, in_dim)\n",
    "#         W = self.weight.unfold(0, D, 1)\n",
    "\n",
    "#         # Compute logits\n",
    "#         logits = torch.einsum(\"btd,kd->btk\", x, W)\n",
    "#         logits = logits + self.bias\n",
    "\n",
    "#         return logits\n",
    "\n",
    "# class Toeplitz_FC_RRCGDNet_UNet(nn.Module):\n",
    "#     def __init__(self, n_bins=360):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # ---------------- Encoder ----------------\n",
    "#         self.down1 = Down(1, 64)\n",
    "#         self.down2 = Down(64, 128)\n",
    "#         self.down3 = Down(128, 256)\n",
    "#         self.down4 = Down(256, 512)\n",
    "\n",
    "#         # ---------------- Bottleneck ----------------\n",
    "#         self.mid = ConvBlock(512, 512)\n",
    "\n",
    "#         # ---------------- Decoder ----------------\n",
    "#         self.up4 = Up(512, 512, 256)\n",
    "#         self.up3 = Up(256, 256, 128)\n",
    "#         self.up2 = Up(128, 128, 64)\n",
    "#         self.up1 = Up(64, 64, 64)\n",
    "\n",
    "#         # Collapse UNet channels\n",
    "#         self.out_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "#         # ---------------- Temporal Frontend ----------------\n",
    "#         self.expand1 = nn.Conv1d(256, 512, kernel_size=1)\n",
    "\n",
    "#         self.temporal1 = nn.Sequential(\n",
    "#             nn.Conv1d(512, 512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         self.expand2 = nn.Conv1d(512, 1024, kernel_size=1)\n",
    "\n",
    "#         self.temporal2 = nn.Sequential(\n",
    "#             nn.Conv1d(1024, 1024, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(1024, 1024, kernel_size=3, padding=2, dilation=2),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         # ---------------- Sequence Model ----------------\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=1024,\n",
    "#             hidden_size=256,\n",
    "#             num_layers=2,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.2,\n",
    "#         )\n",
    "\n",
    "#         # ---------------- Toeplitz Pitch Head ----------------\n",
    "#         self.fc = ToeplitzPitchHead(in_dim=512, n_bins=n_bins)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (B, T, 256)  â€” RRCGD features\n",
    "#         returns: (B, T, n_bins)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # (B, 1, 256, T)\n",
    "#         x = x.unsqueeze(1).transpose(2, 3)\n",
    "\n",
    "#         # Encoder\n",
    "#         s1, x = self.down1(x)\n",
    "#         s2, x = self.down2(x)\n",
    "#         s3, x = self.down3(x)\n",
    "#         s4, x = self.down4(x)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         x = self.mid(x)\n",
    "\n",
    "#         # Decoder\n",
    "#         x = self.up4(x, s4)\n",
    "#         x = self.up3(x, s3)\n",
    "#         x = self.up2(x, s2)\n",
    "#         x = self.up1(x, s1)\n",
    "\n",
    "#         # Collapse channels â†’ (B, 256, T)\n",
    "#         x = self.out_conv(x).squeeze(1)\n",
    "\n",
    "#         # Temporal frontend\n",
    "#         x = self.expand1(x)        # (B, 512, T)\n",
    "#         x = self.temporal1(x)\n",
    "\n",
    "#         x = self.expand2(x)        # (B, 1024, T)\n",
    "#         x = self.temporal2(x)\n",
    "\n",
    "#         # Sequence modeling\n",
    "#         x = x.transpose(1, 2)      # (B, T, 1024)\n",
    "#         x, _ = self.gru(x)         # (B, T, 512)\n",
    "\n",
    "#         # Toeplitz FC pitch head\n",
    "#         logits = self.fc(x)        # (B, T, n_bins)\n",
    "\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9fb8de-e478-43f0-bd69-537f2ea713ae",
   "metadata": {},
   "source": [
    "### RMVPE Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8b3bd-0e18-4108-8dd3-dab61bb42253",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "N_CLASS = 360\n",
    "N_MELS=256\n",
    "HOP_LENGTH=20/1000*SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a60547-ba03-4471-a502-08859f74055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RCB\n",
    "class ConvBlockRes(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, momentum=0.01):\n",
    "        super(ConvBlockRes, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=(1, 1),\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=momentum),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=out_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=(1, 1),\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=momentum),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, (1, 1))\n",
    "            self.is_shortcut = True\n",
    "        else:\n",
    "            self.is_shortcut = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_shortcut:\n",
    "            return self.conv(x) + self.shortcut(x)\n",
    "        else:\n",
    "            return self.conv(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ade1d-435e-462e-9bb5-b55a89abae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REB\n",
    "class ResEncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, n_blocks=1, momentum=0.01):\n",
    "        super(ResEncoderBlock, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.conv = nn.ModuleList()\n",
    "        self.conv.append(ConvBlockRes(in_channels, out_channels, momentum))\n",
    "        for i in range(n_blocks - 1):\n",
    "            self.conv.append(ConvBlockRes(out_channels, out_channels, momentum))\n",
    "        self.kernel_size = kernel_size\n",
    "        if self.kernel_size is not None:\n",
    "            self.pool = nn.AvgPool2d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_blocks):\n",
    "            x = self.conv[i](x)\n",
    "        if self.kernel_size is not None:\n",
    "            return x, self.pool(x)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b492df-bbf7-4b9e-9ae0-0e0792347be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, n_blocks=1, momentum=0.01):\n",
    "        super(ResDecoderBlock, self).__init__()\n",
    "        out_padding = (0, 1) if stride == (1, 2) else (1, 1)\n",
    "        self.n_blocks = n_blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=(3, 3),\n",
    "                               stride=stride,\n",
    "                               padding=(1, 1),\n",
    "                               output_padding=out_padding,\n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=momentum),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        self.conv2.append(ConvBlockRes(out_channels * 2, out_channels, momentum))\n",
    "        for i in range(n_blocks-1):\n",
    "            self.conv2.append(ConvBlockRes(out_channels, out_channels, momentum))\n",
    "\n",
    "    def forward(self, x, concat_tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.cat((x, concat_tensor), dim=1)\n",
    "        for i in range(self.n_blocks):\n",
    "            x = self.conv2[i](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca709a-2372-4f55-9d91-d1e3770aa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder layers\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, in_size, n_encoders, kernel_size, n_blocks, out_channels=16, momentum=0.01):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_encoders = n_encoders\n",
    "        self.bn = nn.BatchNorm2d(in_channels, momentum=momentum)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.latent_channels = []\n",
    "        for i in range(self.n_encoders):\n",
    "            self.layers.append(ResEncoderBlock(in_channels, out_channels, kernel_size, n_blocks, momentum=momentum))\n",
    "            self.latent_channels.append([out_channels, in_size])\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "            in_size //= 2\n",
    "        self.out_size = in_size\n",
    "        self.out_channel = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat_tensors = []\n",
    "        x = self.bn(x)\n",
    "        for i in range(self.n_encoders):\n",
    "            _, x = self.layers[i](x)\n",
    "            concat_tensors.append(_)\n",
    "        return x, concat_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c270de3-0410-4f7d-bc4c-a9f10b5b1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate layers\n",
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_inters, n_blocks, momentum=0.01):\n",
    "        super(Intermediate, self).__init__()\n",
    "        self.n_inters = n_inters\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(ResEncoderBlock(in_channels, out_channels, None, n_blocks, momentum))\n",
    "        for i in range(self.n_inters-1):\n",
    "            self.layers.append(ResEncoderBlock(out_channels, out_channels, None, n_blocks, momentum))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_inters):\n",
    "            x = self.layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513014bc-93f7-4d45-96bf-1127ce7a51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder layers\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, n_decoders, stride, n_blocks, momentum=0.01):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.n_decoders = n_decoders\n",
    "        for i in range(self.n_decoders):\n",
    "            out_channels = in_channels // 2\n",
    "            self.layers.append(ResDecoderBlock(in_channels, out_channels, stride, n_blocks, momentum))\n",
    "            in_channels = out_channels\n",
    "\n",
    "    def forward(self, x, concat_tensors):\n",
    "        for i in range(self.n_decoders):\n",
    "            x = self.layers[i](x, concat_tensors[-1-i])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e28c1-2882-4c8c-85d8-aaee1396dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip hidden layers\n",
    "class TimbreFilter(nn.Module):\n",
    "    def __init__(self, latent_rep_channels):\n",
    "        super(TimbreFilter, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for latent_rep in latent_rep_channels:\n",
    "            self.layers.append(ConvBlockRes(latent_rep[0], latent_rep[0]))\n",
    "\n",
    "    def forward(self, x_tensors):\n",
    "        out_tensors = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out_tensors.append(layer(x_tensors[i]))\n",
    "        return out_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5e1a6-e7f8-4b3f-a341-18cf26d87344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet model defined\n",
    "class DeepUnet(nn.Module):\n",
    "    def __init__(self, kernel_size, n_blocks, en_de_layers=5, inter_layers=4, in_channels=1, en_out_channels=16):\n",
    "        super(DeepUnet, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, N_MELS, en_de_layers, kernel_size, n_blocks, en_out_channels)\n",
    "        self.intermediate = Intermediate(self.encoder.out_channel // 2, self.encoder.out_channel, inter_layers, n_blocks)\n",
    "        self.tf = TimbreFilter(self.encoder.latent_channels)\n",
    "        self.decoder = Decoder(self.encoder.out_channel, en_de_layers, kernel_size, n_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, concat_tensors = self.encoder(x)\n",
    "        x = self.intermediate(x)\n",
    "        concat_tensors = self.tf(concat_tensors)\n",
    "        x = self.decoder(x, concat_tensors)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055476e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, num_layers):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_features, hidden_features, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gru(x)[0]\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_features, hidden_features, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lstm(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484403e6-8fc0-4cae-84d3-f1f40f18cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline of the RMVPE\n",
    "class E2E(nn.Module):\n",
    "    def __init__(self, hop_length, n_blocks, n_gru, kernel_size, en_de_layers=5, inter_layers=4, in_channels=1,\n",
    "                 en_out_channels=16):\n",
    "        super(E2E, self).__init__()\n",
    "        self.unet = DeepUnet(kernel_size, n_blocks, en_de_layers, inter_layers, in_channels, en_out_channels)\n",
    "        self.cnn = nn.Conv2d(en_out_channels, 3, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        if n_gru:\n",
    "            self.fc = nn.Sequential(\n",
    "                BiGRU(3*N_MELS, 256, n_gru),\n",
    "                nn.Linear(512, N_CLASS),\n",
    "                nn.Dropout(0.25),\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(3 * N_MELS, N_CLASS),\n",
    "                nn.Dropout(0.25),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 256)\n",
    "        \n",
    "        # --- FIXED: Avoid shadowing 'F' ---\n",
    "        B, T, freq_dim = x.shape # Rename F to freq_dim\n",
    "        padding_needed = (16 - (T % 16)) % 16\n",
    "        \n",
    "        if padding_needed > 0:\n",
    "            # Pad the Time dimension (dim 1)\n",
    "            x = F.pad(x, (0, 0, 0, padding_needed))\n",
    "        \n",
    "        x = x.unsqueeze(1) # (B, 1, T_padded, 256)\n",
    "        x = self.cnn(self.unet(x)) # (B, 3, T_padded, 256)\n",
    "        \n",
    "        x = x.transpose(1, 2).flatten(-2) # (B, T_padded, 768)\n",
    "\n",
    "        if len(self.fc) >= 1: \n",
    "             x = self.fc(x)\n",
    "\n",
    "        # --- Crop back to original length ---\n",
    "        if padding_needed > 0:\n",
    "            x = x[:, :T, :]\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d7c73-f3d1-4815-9d60-feb497a7410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_BLOCKS=4\n",
    "N_GRU=1\n",
    "KERNEL_SIZE=(2,2)\n",
    "model = E2E(\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_gru=N_GRU,\n",
    "    kernel_size=KERNEL_SIZE).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61f922-c07a-4962-b160-6bb6750b539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RRCGDNet_UNet based model definition\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = RRCGDNet_UNet().to(device)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),lr=2e-5,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01a363-a66e-4cf2-a0a7-9f5b39ec3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# x = torch.randn(1, 200, 256)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y = model(x)\n",
    "\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f4b7d-bb9d-4391-900c-cb146f987377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, FREQ = 2, 100, 256\n",
    "# N_BINS = 360\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Toeplitz_FC_RRCGDNet_UNet(n_bins=N_BINS)\n",
    "# x = torch.randn(B, T, FREQ)\n",
    "# y = model(x)\n",
    "# print(y.shape)  # must be (2, 100, 360)\n",
    "\n",
    "# y.mean().backward()\n",
    "# assert model.fc.weight.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f2b7a-334b-4003-b678-43c55349f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),lr=2e-5,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a1b7a-cf56-490b-8457-b3aae858ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e869ed-2f85-44c2-be32-3ec58c8037a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8016c84-f676-4755-bbbd-55fb9849fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping based on metric\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, min_delta=1e-3):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = 0.0\n",
    "        self.count = 0\n",
    "        self.stop = False\n",
    "\n",
    "    def step(self, rpa):\n",
    "        # Improvement only if RPA increases meaningfully\n",
    "        if rpa > self.best + self.min_delta:\n",
    "            self.best = rpa\n",
    "            self.count = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                self.stop = True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136f978-4897-4fd4-ad4f-0064a4ce26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, best_val_oa, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    model_state = (\n",
    "        model.module.state_dict()\n",
    "        if isinstance(model, torch.nn.DataParallel)\n",
    "        else model.state_dict()\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model_state,\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_oa\": best_val_oa,\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, device):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model.module.load_state_dict(ckpt[\"model_state\"])\n",
    "    else:\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_val_oa = ckpt[\"best_val_oa\"]\n",
    "\n",
    "    return start_epoch, best_val_oa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789eee0d-f0c9-48fc-8f83-897a313d6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def log_epoch_metrics_txt(\n",
    "    filepath,\n",
    "    epoch,\n",
    "    train_loss,\n",
    "    val_loss,\n",
    "    vrr,\n",
    "    rpa,\n",
    "    oa,\n",
    "    mode=\"a\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs one epoch of training/validation metrics to a text file.\n",
    "\n",
    "    filepath : str\n",
    "        Path to .txt log file\n",
    "    epoch : int\n",
    "    train_loss : float\n",
    "    val_loss : float\n",
    "    vrr : float\n",
    "    rpa : float\n",
    "    oa : float\n",
    "    mode : str\n",
    "        \"a\" = append (default), \"w\" = overwrite\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    with open(filepath, mode) as f:\n",
    "        # write header if file is new or overwritten\n",
    "        if mode == \"w\":\n",
    "            f.write(\n",
    "                \"Epoch\\tTrainLoss\\tValLoss\\tVRR\\tRPA\\tOA\\n\"\n",
    "            )\n",
    "\n",
    "        f.write(\n",
    "            f\"{epoch:03d}\\t\"\n",
    "            f\"{train_loss:.6f}\\t\"\n",
    "            f\"{val_loss:.6f}\\t\"\n",
    "            f\"{vrr:.4f}\\t\"\n",
    "            f\"{rpa:.4f}\\t\"\n",
    "            f\"{oa:.4f}\\n\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64407e0-e549-4b3a-bf88-9bd16fd4af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"checkpoints/model_rmvpe/pitch_smoothed_CE_loss_best_oa/train_metrics_log.txt\"\n",
    "\n",
    "# overwrite old log at start of run\n",
    "log_epoch_metrics_txt(\n",
    "    log_path,\n",
    "    epoch=0,\n",
    "    train_loss=0,\n",
    "    val_loss=0,\n",
    "    vrr=0,\n",
    "    rpa=0,\n",
    "    oa=0,\n",
    "    mode=\"w\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57e7de-49de-418c-a67d-580a9930a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists(log_path))  # must print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8c3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_vrrs, val_rpas, val_oas = [], [], []\n",
    "\n",
    "best_val_oa = 0.0\n",
    "start_epoch = 1\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "LAST_PATH=\"checkpoints/model_rmvpe/pitch_smoothed_CE_loss_best_oa/last.pt\"\n",
    "# ---- Resume if exists ----\n",
    "if os.path.exists(LAST_PATH):\n",
    "    start_epoch, best_val_oa = load_checkpoint(\n",
    "        LAST_PATH, model, optimizer, device\n",
    "    )\n",
    "\n",
    "# ---- Training loop ----\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "\n",
    "    # ---- Train ----\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, device\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ---- Validate (loss) ----\n",
    "    val_loss = validate_with_loss(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # ---- Validate (mir_eval metrics) ----\n",
    "    metrics = evaluate_full(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "    vrr = metrics[\"VRR\"]\n",
    "    rpa = metrics[\"RPA\"]\n",
    "    oa  = metrics[\"OA\"]\n",
    "\n",
    "    val_vrrs.append(vrr)\n",
    "    val_rpas.append(rpa)\n",
    "    val_oas.append(oa)\n",
    "# ---- ðŸ”´ ADD THIS LINE (TXT LOGGING) ----\n",
    "    log_epoch_metrics_txt(\n",
    "        log_path,\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        vrr,\n",
    "        rpa,\n",
    "        oa,\n",
    "        mode=\"a\"\n",
    "    )\n",
    "    # ---- Scheduler (OA-driven) ----\n",
    "    scheduler.step(oa)\n",
    "\n",
    "    # ---- Save last checkpoint ----\n",
    "    save_checkpoint(\n",
    "        epoch, model, optimizer, best_val_oa,\n",
    "        \"checkpoints/model_rmvpe/pitch_smoothed_CE_loss_best_oa/last.pt\"\n",
    "    )\n",
    "\n",
    "    # ---- Save best model (OA only) ----\n",
    "    if oa > best_val_oa:\n",
    "        best_val_oa = oa\n",
    "        save_checkpoint(\n",
    "            epoch, model, optimizer, best_val_oa,\n",
    "            \"checkpoints/model_rmvpe/pitch_smoothed_CE_loss_best_oa/best_oa.pt\"\n",
    "        )\n",
    "\n",
    "    # ---- Logging ----\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"Train {train_loss:.6f} | \"\n",
    "        f\"Val {val_loss:.6f} | \"\n",
    "        f\"VRR {vrr:.3f} | RPA {rpa:.3f} | OA {oa:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- Early stopping (same signal as scheduler) ----\n",
    "    early_stopper.step(oa)\n",
    "    if early_stopper.stop:\n",
    "        print(\"Early stopping happened!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f28ff-ede5-4013-aed4-a8894b0d2dc2",
   "metadata": {},
   "source": [
    "Plots and inference on test data and metrics cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b742ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# ---- Loss ----\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ---- Metrics ----\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, val_vrrs, label=\"VRR\")\n",
    "plt.plot(epochs, val_rpas, label=\"RPA\")\n",
    "plt.plot(epochs, val_oas, label=\"OA\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Metrics\")\n",
    "plt.ylim(0,1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/training_curves.png\", dpi=200)  # SAVE FIRST\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddcfb0-76ab-4847-be23-ab8a4a5a7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ema(x, alpha=0.3):\n",
    "    y = np.zeros_like(x, dtype=float)\n",
    "    y[0] = x[0]\n",
    "    for i in range(1, len(x)):\n",
    "        y[i] = alpha * x[i] + (1 - alpha) * y[i-1]\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9a5bc-5c59-438e-8b8f-ac8f499374d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_s = ema(train_losses, alpha=0.3)\n",
    "val_losses_s   = ema(val_losses, alpha=0.3)\n",
    "\n",
    "val_vrrs_s = ema(val_vrrs, alpha=0.3)\n",
    "val_rpas_s = ema(val_rpas, alpha=0.3)\n",
    "val_oas_s  = ema(val_oas,  alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87137e61-7a64-44f6-ad0a-b73f58e18e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "# ---- Loss ----\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(epochs, ema(train_losses, 0.3), label=\"Train Loss\")\n",
    "plt.plot(epochs, ema(val_losses, 0.3), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss (EMA)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ---- Metrics ----\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(epochs, ema(val_vrrs, 0.3), label=\"VRR\")\n",
    "plt.plot(epochs, ema(val_rpas, 0.3), label=\"RPA\")\n",
    "plt.plot(epochs, ema(val_oas,  0.3), label=\"OA\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title(\"Validation Metrics (EMA)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/smooth_training_curves.png\", dpi=200)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_full(model, test_loader, device)\n",
    "\n",
    "test_vrr = metrics[\"VRR\"]\n",
    "test_rpa = metrics[\"RPA\"]\n",
    "test_oa  = metrics[\"OA\"]\n",
    "\n",
    "print(\n",
    "    f\"[TEST] VRR={test_vrr:.3f} | \"\n",
    "    f\"RPA={test_rpa:.3f} | OA={test_oa:.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63d32f-0e95-42a1-acf8-76cf10970a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            total += p.numel()\n",
    "    return total\n",
    "\n",
    "n_params = model_summary(model)\n",
    "print(f\"Total trainable parameters: {n_params:,}\")\n",
    "\n",
    "with open(\"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/model_stats.txt\", \"w\") as f:\n",
    "    f.write(f\"Total trainable parameters: {n_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c362ec-799c-48d6-93c7-cad5e630f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_test_results(\n",
    "    test_vrr, test_rpa, test_oa,\n",
    "    ckpt_dir=\"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/\"\n",
    "):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    results = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"VRR\": test_vrr,\n",
    "        \"RPA\": test_rpa,\n",
    "        \"OA\": test_oa,\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(ckpt_dir, \"test_results.json\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"[LOGGED] Test results â†’ {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25841b3-30bb-4c00-891c-c9f0fed426ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_results(test_vrr, test_rpa, test_oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd2f6e-922f-4ada-87c0-a6ec8ba5444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# unwrap DataParallel\n",
    "export_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "\n",
    "dummy_input = torch.randn(1, 200, 256).to(device)  # (B,T,F) adjust T if needed\n",
    "\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    dummy_input,\n",
    "    \"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/model.onnx\",\n",
    "    input_names=[\"rrcgd\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"rrcgd\": {1: \"time\"},\n",
    "        \"logits\": {1: \"time\"}\n",
    "    },\n",
    "    opset_version=14\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0c010-f348-4404-8344-e940d36b72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "export_model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "export_model = export_model.to(\"cpu\")   # safer for scripting\n",
    "\n",
    "scripted = torch.jit.script(export_model)\n",
    "scripted.save(\"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/model_torchscript.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181a9b0-6228-4bfd-b0cf-66d79a797f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfd6ed-7e1b-454c-a135-feb7d4d97725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    export_model,\n",
    "    input_size=(1, 200, 256),  # (B,T,F)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88884386-b867-4d72-9fba-c06fc68a8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/summary.txt\", \"w\") as f:\n",
    "    f.write(str(summary(export_model, input_size=(1,200,256))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd23de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_voicing(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    TP = FP = FN = TN = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y, _, lengths in loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            B = X.shape[0]\n",
    "\n",
    "            for b in range(B):\n",
    "                T = lengths[b].item()\n",
    "\n",
    "                gt_bins   = Y[b, :T].cpu().numpy()\n",
    "                pred_bins = preds[b, :T].cpu().numpy()\n",
    "\n",
    "                gt_voiced   = gt_bins > 0\n",
    "                pred_voiced = pred_bins > 0\n",
    "\n",
    "                TP += np.sum(pred_voiced & gt_voiced)\n",
    "                FP += np.sum(pred_voiced & ~gt_voiced)\n",
    "                FN += np.sum(~pred_voiced & gt_voiced)\n",
    "                TN += np.sum(~pred_voiced & ~gt_voiced)\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall    = TP / (TP + FN + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TN\": TN,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_voicing_confusion_matrix(cm, normalize=True):\n",
    "    \"\"\"\n",
    "    cm: 2x2 confusion matrix\n",
    "    normalize: show percentages instead of counts\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm / (cm.sum(axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    im = ax.imshow(cm)\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([\"Unvoiced\", \"Voiced\"])\n",
    "    ax.set_yticklabels([\"Unvoiced\", \"Voiced\"])\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Ground Truth\")\n",
    "    ax.set_title(\"Voicing Confusion Matrix\")\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = cm[i, j]\n",
    "            txt = f\"{val:.2f}\" if normalize else f\"{int(val)}\"\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3002ffe-23df-46c6-b47b-d3843c751965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def voicing_confusion_matrix(voicing_metrics):\n",
    "    TP = voicing_metrics[\"TP\"]\n",
    "    FP = voicing_metrics[\"FP\"]\n",
    "    FN = voicing_metrics[\"FN\"]\n",
    "    TN = voicing_metrics[\"TN\"]\n",
    "\n",
    "    # rows = ground truth, cols = prediction\n",
    "    return np.array([\n",
    "        [TN, FP],\n",
    "        [FN, TP]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ec413",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicing_metrics = evaluate_voicing(model, test_loader, device)\n",
    "\n",
    "print(\n",
    "    f\"[VOICING] \"\n",
    "    f\"Precision={voicing_metrics['Precision']:.3f} | \"\n",
    "    f\"Recall={voicing_metrics['Recall']:.3f} | \"\n",
    "    f\"F1={voicing_metrics['F1']:.3f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Confusion: \"\n",
    "    f\"TP={voicing_metrics['TP']} | \"\n",
    "    f\"FP={voicing_metrics['FP']} | \"\n",
    "    f\"FN={voicing_metrics['FN']} | \"\n",
    "    f\"TN={voicing_metrics['TN']}\"\n",
    ")\n",
    "\n",
    "cm = voicing_confusion_matrix(voicing_metrics)\n",
    "\n",
    "plot_voicing_confusion_matrix(cm, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d7379-464d-4b68-bdcf-51f7555ec9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voicing_scores(voicing_metrics):\n",
    "    labels = [\"Precision\", \"Recall\", \"F1\"]\n",
    "    values = [\n",
    "        voicing_metrics[\"Precision\"],\n",
    "        voicing_metrics[\"Recall\"],\n",
    "        voicing_metrics[\"F1\"],\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.bar(labels, values)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Voicing Metrics\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee5f52-5625-4cc4-9ea8-277e55d3ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voicing_scores(voicing_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edd767-a219-4425-8164-0f3bd5acdf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_voicing_metrics(voicing_metrics, out_path):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(voicing_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c2968-a1d2-48e1-b693-e9203be20c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_voicing_metrics_txt(voicing_metrics, out_path):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"Voicing metrics\\n\")\n",
    "        f.write(\"================\\n\")\n",
    "        f.write(f\"Precision : {float(voicing_metrics['Precision']):.6f}\\n\")\n",
    "        f.write(f\"Recall    : {float(voicing_metrics['Recall']):.6f}\\n\")\n",
    "        f.write(f\"F1-score  : {float(voicing_metrics['F1']):.6f}\\n\")\n",
    "        f.write(\"\\nConfusion matrix counts\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        f.write(f\"TP : {int(voicing_metrics['TP'])}\\n\")\n",
    "        f.write(f\"FP : {int(voicing_metrics['FP'])}\\n\")\n",
    "        f.write(f\"FN : {int(voicing_metrics['FN'])}\\n\")\n",
    "        f.write(f\"TN : {int(voicing_metrics['TN'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d17f1d-261a-408f-ac64-cc1d94e23a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicing_metrics = evaluate_voicing(model, test_loader, device)\n",
    "\n",
    "save_voicing_metrics_txt(\n",
    "    voicing_metrics,\n",
    "    \"checkpoints/model_Toeplitz_FC_RRCGDNet_UNet_fs_by_4/pitch_smoothed_CE_loss_best_oa/voicing_metrics_test.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81b305-ffa5-4aa3-aebe-d4d4253288f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gd_exps)",
   "language": "python",
   "name": "gd_exps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
